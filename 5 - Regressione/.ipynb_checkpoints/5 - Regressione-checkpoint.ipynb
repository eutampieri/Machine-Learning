{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contenuti\n",
    "- [Descrizione del dataset](#Descrizione-del-dataset)\n",
    "- [Pandas](#Pandas)\n",
    "    - [Esplorazione dei dati](#Esplorazione-dei-dati)\n",
    "    - [Preprocessing dei dati](#Preprocessing-dei-dati)\n",
    "    - [Predizione all'ora successiva](#Predizione-all'ora-successiva)\n",
    "    - [Split validation set](#Split-validation-set)\n",
    "- [Regressione in Scikit-Learn](#Regressione-in-Scikit-Learn)\n",
    "- [Ottimizzazione dei parametri](#Ottimizzazione-dei-parametri)\n",
    "- [Test](#Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercitazione 5\n",
    "Nell'esercitazione odierna si vedrà come applicare i regressori *lineare* e *Random Forest* a un problema reale. La lezione è suddivisa in tre parti:\n",
    "\n",
    "- introduzione alla libreria **Pandas**;\n",
    "- introduzione alla regressione in **Scikit-learn**;\n",
    "- risoluzione del problema.\n",
    "\n",
    "Nel corso dell'esercitazione si dovranno individuare le combinazioni di iperparametri che permettono di minimizzare l'RMSE (*Root Mean Squared Error*) sul dataset fornito utilizzando *Cross-Validation* e *Grid Search*.\n",
    "\n",
    "Infine si dovrà verificare l’effettiva capacità di generalizzazione della soluzione trovata sul dataset di test.\n",
    "\n",
    "# Descrizione del dataset\n",
    "\n",
    "Obiettivo di questa esercitazione è quello di creare un modello di regressione in grado di prevedere la quantità di energia prodotta da un impianto fotovoltaico. Il dataset è relativo a un impianto fotovoltaico reale di 960 kWP ubicato a Lecce. Il dataset:\n",
    "\n",
    "- contiene circa 15000 record relativi a medie orarie su un periodo di poco meno di 2 anni (500 giorni);\n",
    "-  la variabile **indipendente** è costituita dal vettore 6-dimensionale [Ora, Data, Temperatura ambiente, Temperatura dei moduli, Irraggiamento dei pannelli inclinati a 3 gradi, Irraggiamento dei pannelli inclinati a 15 gradi];\n",
    "-  la **variabile dipendente** (valore *target*) è costituita dalla produzione in kWH dell'impianto in una determinata ora.\n",
    "\n",
    "Di seguito vengono forniti i riferimenti per ulteriori approfondimenti:\n",
    "- M. Malvoni, M. G. De Giorgi, P. M. Congedo, [*Data on photovoltaic powerforecasting models for Mediterranean climate*](https://www.sciencedirect.com/science/article/pii/S2352340916302773), Data in Brief, 2016.\n",
    "- M. G. De Giorgi, M. Malvoni, P. M. Congedo, [*Comparison of strategies for multi-step ahead photovoltaic power forecasting models based on hybrid group method of data handling networks and least square support vector machine*](https://www.sciencedirect.com/science/article/abs/pii/S0360544216304261), Energy, 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas\n",
    "**Pandas** è una libreria python per la manipolazione e analisi di dati multidimensionali. Fornisce diversi metodi per la visualizzazione e analisi statistica dei dati e pertanto risulta molto utile per condurre un'esplorazione iniziale. È in grado di caricare dataset forniti nei formati più conosciuti ed è in grado di gestire feature eterogenee e serie temporali.\n",
    "\n",
    "Il codice contenuto nella cella seguente esegue l'import della libreria. Per convenzione la libreria è solitamente importata con il nome **pd**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas è in grado di caricare dataset in formato CSV attraverso la funzione **read_csv(...)**. Questa funzione carica in memoria il contenuto del file restituendo una istanza della classe **DataFrame**. A differenza degli **ndarray**, nei **DataFrame** ogni colonna (feature) è identificata da un nome (definito nell'*header* del CSV) e ogni feature può essere di tipo differente.\n",
    "\n",
    "La cella seguente mostra come richiamare **read_csv(...)** per caricare un dataset in memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "filePath = './DBs/SolarPark/train.txt'\n",
    "\n",
    "dataframe = pd.read_csv(filePath)\n",
    "print(type(dataframe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esplorazione dei dati\n",
    "\n",
    "La classe **DataFrame** espone diversi metodi per ottenere informazioni di sintesi riguardanti i dati appena caricati.\n",
    "\n",
    "Il metodo **.info()** permette di stampare a video cardinalità, features e tipi di dati caricati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stampa di informazioni sulle features (colonne): cardinalità e tipo\n",
    "dataframe.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il metodo **.describe()** restituisce un insieme di informazioni statistiche riguardanti le diverse feature. Vengono riportati i valori minimi e massimi così come la media, la deviazione standard e il 25-, 50-, 75- e 100-percentile. Si noti come Jupyter sia in grado di individuare e formattare ad-hoc il risultato di **.describe()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stampa delle statistiche aggregate sulle features\n",
    "dataframe.describe()\n",
    "\n",
    "# Jupyter gestisce direttamente il risultato di describe() presentandolo adeguatamente\n",
    "# Si noti la differenza di output decommentando la seguente riga:\n",
    "#print(dataframe.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allo stesso modo è possibile visualizzare alcuni dei pattern relativi al **DataFrame**. \n",
    "L'output della cella seguente mostrerà alcuni dei primi e ultimi pattern del dataset. Da questa prima esplorazione risulta chiaro che i pattern sono ordinati temporalmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essendo Pandas ampiamente utilizzato in ambito Machine Learning e Data Science, son disponibili *plugin* o customizzazioni di Jupyter per permettere un'esplorazione più interattiva dei dati. Ad esempio, Google Colaboratory offre [un'estensione ad hoc](https://twitter.com/GoogleColab/status/1190349318617583616) per trattare la visualizzazione di **DataFrame**.\n",
    "\n",
    "Inoltre, la classe **DataFrame** offre la possibilità di visualizzare graficamente la distribuzione dei valori delle feature sotto forma di istogrammi. Attraverso la visualizzazione grafica della distribuzione dei valori è possibile individuare tra le varie cose anomalie e la presenza di dati mancanti.\n",
    "\n",
    "La visualizzazione grafica è demandata alla libreria Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizzazione grafica della distribuzione dei valori delle features (istogrammi)\n",
    "dataframe.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante la prima analisi dei dati può essere utile individuare eventuali correlazioni tra le diverse feature. È possibile utilizzare il metodo **.corr()** per calcolare i coefficienti di correlazione. Nella cella seguente verranno stampati a video i coefficienti relativi alla correlazione tra le diverse feature rispetto all'etichetta \"P (kW)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcolo dei coefficienti di correlazione tra le features\n",
    "corr_matrix = dataframe.corr(numeric_only=True)\n",
    "\n",
    "# Stampa dei coefficienti di correlazione tra le features e \"P (kW)\"\n",
    "print(corr_matrix[\"P (kW)\"].sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing dei dati\n",
    "I dati contenuti nel **DataFrame** appena caricato non possono essere usati direttamente per l'addestramento di un modello di Machine Learning: la presenza di feature eterogenee (stringhe, interi e numeri floating point) deve essere gestita attraverso un'opportuna fase di *preprocessing*. In questa fase si avrà anche la possibilità di estrarre maggiori informazioni dai dati composti (come le date).\n",
    "\n",
    "Ad esempio, la cella seguente mostra come sia possibile gestire le date (attualmente memorizzate sotto forma di stringa) per ottenere informazioni significative. La conversione da stringa a oggetto **Timestamp** è demandata alla funzione **to_datetime(...)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ottiene la stringa Date (colonna 1) del primo pattern\n",
    "datetime_str = dataframe.values[:,1][0]  \n",
    "print('Stringa datetime:', datetime_str)\n",
    "\n",
    "# Conversione da stringa a oggetto Timestamp\n",
    "datetime = pd.to_datetime(datetime_str, format='%m/%d/%y %I:%M %p')\n",
    "print(type(datetime))\n",
    "print('Datetime nel formato \"anno-mese-giorno orario\":', datetime)\n",
    "\n",
    "# Stampa di informazioni relative alla data\n",
    "print('Giorno dell\\'anno:', datetime.timetuple().tm_yday)\n",
    "print('Mese dell\\'anno:', datetime.month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per poter addestrare un modello di Machine Learning è necessario convertire i dati del **DataFrame** in valori numerici e memorizzarli in un **ndarray**. Durante questa procedura sarà possibile estrarre maggiori informazioni, come visto nella cella precedente.\n",
    "\n",
    "La cella successiva converte il **DataFrame** iniziale in un **ndarray** e opzionalmente esegue alcune operazioni per estrarre alcune informazioni aggiuntive dalla data.\n",
    "\n",
    "Si noti l'uso dell'operatore [**_c\\[...\\]**](https://docs.scipy.org/doc/numpy/reference/generated/numpy.c_.html): questo permette di concatenare più **ndarray** ottenendone uno nuovo che contiene lo stesso numero di righe ma le cui colonne sono la concatenazione di quelle degli **ndarray** originali. Per comparazione, [**concatenate(...)**](https://docs.scipy.org/doc/numpy/reference/generated/numpy.concatenate.html) invece permette di concatenare più **ndarray** per riga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_time_data = True\n",
    "\n",
    "if include_time_data:\n",
    "    datetime_strings = dataframe.values[:,1]\n",
    "    \n",
    "    # Converte tutte le stringhe in oggetti Timestamp\n",
    "    datetimes = [pd.to_datetime(datetime, format='%m/%d/%y %I:%M %p') for datetime in datetime_strings]\n",
    "    \n",
    "    # Estrae alcune informazioni dagli oggetti Timestamp\n",
    "    month = [datetime.month for datetime in datetimes]  # Meglio usare il mese se split temporale sequenziale\n",
    "    yday = [datetime.timetuple().tm_yday for datetime in datetimes]  # Giorno dell'anno (se split random)\n",
    "    # print(month)\n",
    "    # print(yday)\n",
    "\n",
    "# feature_x conterrà tutti i campi (esclusi Time Frame, Data e Potenza prodotta)\n",
    "# Reminder: colonna 0 = Time Frame, colonna 1 = date, colonna 6 = potenza prodotta\n",
    "feature_x = dataframe.values[:,2:6]\n",
    "\n",
    "# Se include_time_data è True, include nel dataset anche le features relative alla data\n",
    "# Nota: come orario è meglio utilizzare l'ordinale della colonna Time Frame (evita alcuni errori sui dati)\n",
    "if include_time_data:\n",
    "    #feature_x = np.c_[feature_x, dataframe.values[:,0]] # Include solamente il Time Frame\n",
    "    feature_x = np.c_[feature_x, dataframe.values[:,0], month] # Include il Time Frame e il mese\n",
    "    #feature_x = np.c_[feature_x, dataframe.values[:,0], yday] # Include il Time Frame e il giorno dell'anno\n",
    "    \n",
    "# Estrae le etichette dal dataframe\n",
    "feature_y = dataframe.values[:,6]   # Colonna 6: \"P (kW)\"\n",
    "\n",
    "# Cast di tutti i valori a float32\n",
    "feature_x = feature_x.astype(np.float32)\n",
    "feature_y = feature_y.astype(np.float32)\n",
    "\n",
    "# Alcune stampe di riepilogo\n",
    "#print('Shape del dataset:', feature_x.shape)\n",
    "#print('Primo pattern del dataset:', feature_x[0])\n",
    "#print('Shape delle etichette:', feature_y.shape)\n",
    "#print('Etichetta del primo pattern:', feature_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predizione all'ora successiva\n",
    "In base al task da risolvere può essere richiesto di predire la quantità di energia prodotta nella stessa ora (ovvero l'etichetta dello stesso pattern) oppure di predire l'energia prodotta nell'ora successiva (ovvero l'etichetta del pattern successivo). Quest'ultimo scenario (*predict\\_ahead*) risulta più interessante e può essere gestito assegnando a ogni pattern l'etichetta del pattern successivo.\n",
    "\n",
    "Si noti come fino a questo punto i pattern del dataset non siano stati mescolati. In questa maniera è possibile modificare il dataset per allineare ogni pattern all'etichetta del pattern successivo utilizzando le istruzioni contenute nella seguente cella (in questo caso l'ultimo pattern verrà eliminato dal dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se predict_ahead è True i dati saranno preparati per lo scenario \"predict_ahead\"\n",
    "predict_ahead = False\n",
    "\n",
    "if predict_ahead:\n",
    "    # Sfasa i dati: le prediction sono quelle dell'ora successiva\n",
    "    data_x = feature_x[0:-1,:]\n",
    "    data_y = feature_y[1:]\n",
    "else:\n",
    "    data_x = feature_x\n",
    "    data_y = feature_y\n",
    "\n",
    "print(len(data_x), len(data_y))    \n",
    "# Nota: sfasando i dati si perde l'ultimo pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split validation set\n",
    "È possibile suddividere il dataset originale in train e validation set secondo due differenti strategie:\n",
    "- la suddivisione casuale consiste nel selezionare casualmente i pattern da inserire nei due differenti set;\n",
    "- la suddivisione temporale consiste nel suddividere il set originale in due parti non sovrapposte temporalmente.\n",
    "\n",
    "Considerato che i pattern sono ordinati temporalmente, è possibile eseguire la seconda strategia operando un semplice *slicing* dei dati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_portion = 0.5\n",
    "split_random = False\n",
    "\n",
    "if split_random:\n",
    "    # Split casuale di train e validation set\n",
    "    train_x, validation_x, train_y, validation_y = train_test_split(data_x, data_y, test_size=valid_portion, random_state=1)\n",
    "else:    \n",
    "    # Split temporale\n",
    "    train_x, validation_x, train_y, validation_y = train_test_split(data_x, data_y, test_size=valid_portion, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressione in Scikit-Learn\n",
    "L'obiettivo è quello di predire la quantità di energia prodotta in una certa fascia oraria (o in quella immediatamente successiva). La libreria Scikit-learn mette a disposizione diversi algoritmi per la regressione. Durante questa esercitazione verrà fatto uso di [**LinearRegression**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) e di [**RandomForestRegressor**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html).\n",
    "\n",
    "Queste classi condividono la API con quella dei classificatori. Pertanto, per addestrare un'istanza di un regressore sarà sufficiente richiamare il metodo **.fit(...)** mentre per ottenere le predizioni sarà necessario richiamare il metodo **.predict(...)**. L'obiettivo è quello di minimizzare l'RMSE, ovvero lo scostamento tra il valore predetto e quello reale.\n",
    "\n",
    "Nella cella seguente è fornito il codice per creare e addestrare un'istanza di [**LinearRegression**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addestramento di un LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(train_x, train_y)\n",
    "\n",
    "# Ottenimento delle predizioni\n",
    "train_y_predicted = lin_reg.predict(train_x)\n",
    "\n",
    "# Calcolo del RMSE\n",
    "rmse = np.sqrt(mean_squared_error(train_y, train_y_predicted))\n",
    "print('Train RMSE: ', rmse) \n",
    "\n",
    "# Ottenimento delle predizioni (validation) e calcolo RMSE\n",
    "validation_y_predicted = lin_reg.predict(validation_x)\n",
    "rmse = np.sqrt(mean_squared_error(validation_y, validation_y_predicted))\n",
    "print('Validation RMSE: ', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allo stesso modo è possibile creare e addestrare un [**RandomForestRegressor**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addestramento di un RandomForestRegressor\n",
    "forest_reg = RandomForestRegressor(n_estimators=10)   \n",
    "forest_reg.fit(train_x, train_y)\n",
    "\n",
    "# Ottenimento delle predizioni\n",
    "train_y_predicted = forest_reg.predict(train_x)\n",
    "\n",
    "# Calcolo del RMSE\n",
    "rmse = np.sqrt(mean_squared_error(train_y, train_y_predicted))\n",
    "print('Train RMSE: ', rmse)\n",
    "\n",
    "# Ottenimento delle predizioni (validation) e calcolo RMSE\n",
    "validation_y_predicted = forest_reg.predict(validation_x)\n",
    "rmse = np.sqrt(mean_squared_error(validation_y, validation_y_predicted))\n",
    "print('Validation RMSE: ', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funzione **.score(...)**, presente anche nella API degli *estimator*, restituisce un punteggio riguardante la capacità del classificatore/regressore nel predire i valori corretti. Per i classificatori questa funzione restitusce l'accuratezza sul dataset dato. Per i regressori restituisce il coefficiente $R^{2}$, ovvero un valore compreso tra 0 e 1 tanto più alto quanto è migliore la predizione restituita. Il valore di $R^{2}$ è statisticamente valido solo per regressione lineare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('R2 score:', lin_reg.score(validation_x, validation_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partire dalle predizione ottenuta nella cella precedente è possibile visualizzare la distribuzione degli errori sul validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola l'errore come scostamento delle predizioni dal valore reale\n",
    "errors = np.abs(validation_y - validation_y_predicted) \n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.title(\"Distribuzione degli errori\")\n",
    "plt.hist(x = errors, bins=50)\n",
    "plt.show()\n",
    "\n",
    "# Visualizza l'andamento reale e quello predetto\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.title(\"Confronto tra valori reali e prediction\")\n",
    "plt.plot(validation_y[0:1000], label='Real')\n",
    "plt.plot(validation_y_predicted[0:1000], label='Prediction')\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizio\n",
    "Si consideri ora lo scenario *predict ahead*, in cui viene richiesto di predire la potenza prodotta dall'impianto fotovoltaico durante l'ora successiva. Si metta in pratica quanto appreso nei passi precedenti al fine di identificare il regressore e i relativi iperparametri in grado di ottimizzare l'RMSE sul dataset di test.\n",
    "\n",
    "È possibile procedere considerando la possibilità di eseguire un'opportuna *Cross-Validation* utilizzando fold composte sia da periodi temporali ordinati (consigliabile) che fold con pattern selezionati casualmente. Qualora si desideri procedere senza *Cross-Validation* si consiglia di suddividere il dataset originale per ottenere un set di training e uno di validation.\n",
    "\n",
    "Si consideri inoltre che sia [**GridSearchCV(...)**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) che [**cross_val_score(...)**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) utilizzano internamente il meotodo **.score(...)** dell'istanza del regressore per ottenere il punteggio $R^{2}$ relativo alla bontà della regressione. Al fine di utilizzare MSE come punteggio è sufficiente istanziare **GridSearchCV(...)** con il parametro *scoring* impostato al valore *'neg_mean_squared_error'* (stringa). Lo stesso si applica a **cross_val_score(...)**. Per poter ottenere l'RMSE a partire dai punteggi ottenuti durante la *grid search* è sufficiente applicare **np.sqrt(...)** a *-gscv.best\\_score\\_* e *-gscv.cv\\_results\\_\\['mean_test_score'\\]*.\n",
    "\n",
    "Si inizi valutando quali dati inserire nel dataset (eventualmente ri-eseguendo la fase di caricamento e *preprocessing*) per poi preparare i dati così ottenuti allo scenario *predict ahead*.\n",
    "\n",
    "Nell'ottimizzazione della soluzione considerare:\n",
    "\n",
    "- quali feature includere nei pattern\n",
    "- considerare l'eventuale circolarizzazione delle date (ora del giorno, giorno nell'anno, mese). Vedi: https://towardsdatascience.com/cyclical-features-encoding-its-about-time-ce23581845ca\n",
    "- l'uso di un regressore scelto tra lineare e random forest\n",
    "- l'eventuale combinazione di due o più regressori\n",
    "- l'eventuale cleaning dei dati ritenuti anomali\n",
    "\n",
    "NOTA:\n",
    "- non è possibile usare l'energia prodotta al tempo t per stimare quella prodotta al tempo t+1. In altre parole non si può includere nel vettore x il valore di y al tempo corrente. Tra l'altro non sarebbe possibile nemmeno fare le prediction sul test_x (non essendo fornito test_y).\n",
    "- non si possono utilizzare regressori diversi dai due sopraindicati\n",
    "- non si possono estendere i dati (augmentation), ma si possono ridurre (cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ottimizzazione con Cross-Validation\n",
    "# ...\n",
    "\n",
    "# Metodo consigliato per la Grid Search:\n",
    "# Con shuffle=False si otterranno fold temporalmente ordinate\n",
    "# (se i pattern non sono stati precedentemente mescolati)\n",
    "# experiment_gscv = GridSearchCV(reg, param_grid, \\\n",
    "#                                cv=KFold(n_splits=4, shuffle=False), \\\n",
    "#                                scoring='neg_mean_squared_error')\n",
    "\n",
    "# Stampa risultati\n",
    "# ...\n",
    "# print('RMSE medio per combinazione:\\n', np.sqrt(-experiment_gscv.cv_results_['mean_test_score']))\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica dei parametri trovati (utilizzando cross_val_score o il validation set)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "Si addestri il regressore desiderato utilizzando gli iperparametri trovati nell'esercizio precedente. Il codice contenuto nella cella seguente userà tale regressore per predire la potenza prodotta nell'ora successiva a partire dai pattern del dataset di test. I valori predetti verranno memorizzati su un file di testo che dovrà essere caricato sul sito della competizione per misurarne l'RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Esercizio 2: valutazione su test set\n",
    "\n",
    "train_path = './DBs/SolarPark/train.txt'\n",
    "test_path = './DBs/SolarPark/test.txt'\n",
    "result_path = 'Es5Predictions.txt'\n",
    "\n",
    "# Caricamento dei pattern di train\n",
    "train_dataframe = pd.read_csv(train_path)\n",
    "# ...\n",
    "\n",
    "# Estrazione delle informazioni dagli oggetti Timestamp\n",
    "train_month = [datetime.month for datetime in ...]\n",
    "# ...\n",
    "\n",
    "# Cast di tutti i valori a float32\n",
    "# ...\n",
    "\n",
    "# Caricamento dei pattern di test\n",
    "# Ricordarsi che i valori della colonna 6 (y) non sono disponibili!\n",
    "test_dataframe = pd.read_csv(test_path)\n",
    "# ...\n",
    "\n",
    "# Estrazione delle informazioni dagli oggetti Timestamp\n",
    "test_month = [datetime.month for datetime in ...]\n",
    "# ...\n",
    "\n",
    "# Cast di tutti i valori a float32\n",
    "# ...\n",
    "\n",
    "# Creazione e addestramento del classificatore\n",
    "reg = ... \n",
    "\n",
    "print('Addestramento in corso ...')\n",
    "reg.fit(train_x, train_y)\n",
    "\n",
    "# Salvataggio delle predictions\n",
    "predictions = reg.predict(test_x)\n",
    "np.savetxt(result_path, predictions)\n",
    "print('Ok')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
